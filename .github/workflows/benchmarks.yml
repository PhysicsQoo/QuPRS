name: Python CI & Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  # ====================================================================
  # JOB 1: Build and save a new performance baseline when code is merged into main.
  # ====================================================================
  build-and-save-baseline:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - name: Checkout main branch code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: pip install .[dev]

      - name: Run benchmark and save results as JSON
        # No path needed; pytest will auto-discover the 'test' directory.
        run: pytest --benchmark-only --benchmark-json=main_baseline.json

      - name: Upload benchmark baseline as artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline-${{ matrix.python-version }}
          path: main_baseline.json

  # ====================================================================
  # JOB 2: For Pull Requests, run tests and compare performance against the main branch baseline.
  # ====================================================================
  test-and-benchmark-pr:
    if: github.event_name == 'pull_request'
    permissions:
      pull-requests: write # Required to post comments on the PR
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      # --- Step 1: Prepare the Pull Request Environment ---
      - name: Checkout PR code
        uses: actions/checkout@v4

      - name: Setup Python and Install PR Dependencies
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      - name: Install PR dependencies
        run: pip install .[dev]

      - name: Run Unit Tests on PR
        # No path needed for auto-discovery.
        run: pytest -n auto -v

      - name: Run Benchmark on PR code
        # No path needed for auto-discovery.
        run: pytest --benchmark-only --benchmark-json=pr_benchmark.json

      # --- Step 2: Obtain the Baseline from the main branch (with fallback) ---
      - name: Download baseline artifact from the main branch
        id: get_baseline
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: ${{ github.workflow }}
          branch: main
          name: benchmark-baseline-${{ matrix.python-version }}
        continue-on-error: true

      - name: Generate baseline on-the-fly if not found
        if: steps.get_baseline.conclusion == 'failure'
        run: |
          echo "‚ö†Ô∏è WARNING: Baseline artifact not found. Attempting to generate from main branch on-the-fly."
          
          echo "INFO: Cloning main branch into 'main_code' directory..."
          git clone "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}" main_code --branch main --single-branch

          cd main_code

          echo "INFO: Creating and activating a separate venv for the main branch..."
          python -m venv .venv
          source .venv/bin/activate
          
          echo "INFO: Installing dependencies for main branch inside its venv..."
          pip install --upgrade pip
          pip install .[dev]

          echo "INFO: Running benchmark on main branch inside its venv..."
          # Here, the path '../test' IS REQUIRED.
          # This is because we are currently inside the 'main_code' directory,
          # and pytest needs to be told to look in the parent directory for the 'test' folder.
          pytest --benchmark-only --benchmark-json=../main_baseline.json ../test || true
          
          cd ..
          echo "INFO: Deactivated and finished with the temporary venv."

      # --- Step 3: Compare, Report, and Comment ---
      - name: Compare benchmarks and generate final JSON report
        run: |
          if [ ! -f "main_baseline.json" ]; then
            echo "‚ùå ERROR: Failed to obtain or generate baseline. Creating an empty one."
            echo '{"benchmarks": []}' > main_baseline.json
          fi

          echo "INFO: Comparing PR benchmark against the obtained baseline."
          # No path needed; comparison is done via JSON files, not by re-running tests.
          pytest --benchmark-compare=main_baseline.json \
                 --benchmark-storage=file://. \
                 --benchmark-compare-fail=mean:10% \
                 --benchmark-json=comparison_results.json

      - name: Generate benchmark markdown report
        run: python src/generate_benchmark_report.py

      - name: Upload benchmark report artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ matrix.python-version }}
          path: |
            benchmark_report.md
            comparison_results.json

      - name: Comment benchmark results on PR
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          path: benchmark_report.md
          header: "üîç Benchmark Comparison Report (Python ${{ matrix.python-version }})"