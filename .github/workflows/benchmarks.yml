name: Python CI & Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  # ====================================================================
  # JOB 1: Build and save a new performance baseline when code is merged into main.
  # This job's sole responsibility is to run on the main branch and store
  # its performance benchmark results as a workflow artifact.
  # ====================================================================
  build-and-save-baseline:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - name: Checkout main branch code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: pip install .[dev]

      - name: Run benchmark and save results as JSON
        run: pytest --benchmark-only --benchmark-json=main_baseline.json

      - name: Upload benchmark baseline as artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline-${{ matrix.python-version }}
          path: main_baseline.json

  # ====================================================================
  # JOB 2: For Pull Requests, run tests and compare performance against the main branch baseline.
  # This version is simplified to be robust: it fetches the baseline, and if
  # the baseline doesn't exist, it skips the comparison instead of trying
  # to generate it on-the-fly, thus avoiding all environment conflicts.
  # ====================================================================
  test-and-benchmark-pr:
    if: github.event_name == 'pull_request'
    permissions:
      pull-requests: write
      actions: read
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      # --- Step 1: Prepare the PR Environment ---
      - name: Checkout PR code
        uses: actions/checkout@v4

      - name: Setup Python and Install PR Dependencies
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      - name: Install PR dependencies
        run: pip install .[dev]

      # --- Step 2: Run Benchmark on the PR ---
      - name: Run Benchmark on PR code
        run: pytest --benchmark-only --benchmark-json=pr_benchmark.json

      # --- Step 3: Attempt to Download the Baseline from Main ---
      - name: Download baseline artifact from the main branch
        id: get_baseline
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: ${{ github.workflow }}
          branch: main
          name: benchmark-baseline-${{ matrix.python-version }}
          # The path where the artifact will be downloaded.
          path: . 
        continue-on-error: true

      # --- Step 4: Compare and Report ---
      - name: Compare benchmarks and generate report
        id: compare_benchmarks # Give this step an ID
        run: |
          # Check if the baseline was successfully downloaded.
          if [ -f "main_baseline.json" ]; then
            echo "‚úÖ Baseline found. Comparing PR against main."
            # Set a flag to indicate that comparison will be performed.
            echo "comparison_performed=true" >> $GITHUB_OUTPUT
            pytest --benchmark-compare=main_baseline.json \
                   --benchmark-storage=file://. \
                   --benchmark-compare-fail=mean:10% \
                   --benchmark-json=comparison_results.json
          else
            echo "‚ö†Ô∏è WARNING: Baseline artifact from main branch not found. Skipping comparison."
            # Set a flag to indicate that comparison was skipped.
            echo "comparison_performed=false" >> $GITHUB_OUTPUT
            # Create an empty result file so the next step doesn't fail.
            echo '{"benchmarks": []}' > comparison_results.json
          fi

      - name: Generate benchmark markdown report
        run: |
          # Pass the comparison status to the report generation script.
          python src/generate_benchmark_report.py --comparison-status ${{ steps.compare_benchmarks.outputs.comparison_performed }}

      - name: Upload benchmark report artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ matrix.python-version }}
          path: |
            benchmark_report.md
            comparison_results.json

      - name: Comment benchmark results on PR
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          path: benchmark_report.md
          header: "üîç Benchmark Comparison Report (Python ${{ matrix.python-version }})"