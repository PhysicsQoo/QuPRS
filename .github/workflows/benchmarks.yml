name: Python CI & Benchmarks

on:
    push:
        branches: [ main ]
    pull_request:
        branches: [ main ]

jobs:
  # ────────────────────────────────────────────────────────────────────────────
  # 1. Unit tests + Benchmark baseline (only runs on push to main branch)
  build-and-save-baseline:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install .[dev]  # Includes pytest, pytest-benchmark, matplotlib, tabulate

      - name: Run tests and save baseline benchmark
        run: |
          pytest --benchmark-only --benchmark-save=main --benchmark-storage=file://./benchmarks
      - name: Upload baseline benchmark artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-main
          path: ./benchmarks/**/*.json

  # ────────────────────────────────────────────────────────────────────────────
  # 2. Unit tests + PR Benchmark comparison (runs on pull requests)
  test-and-benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]

    steps:
      - name: Checkout PR Code
        uses: actions/checkout@v4
        with:
          path: pr_code

      - name: Checkout main branch for baseline
        if: github.event_name == 'pull_request'
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          path: main_code

      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ./pr_code[dev]
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            pip install ./main_code[dev]
          fi
      
      - name: Run unit tests on PR code
        # Set the working directory to pr_code. This ensures that:
        # 1. Pytest correctly discovers modules within the QuPRS package.
        # 2. Relative paths like './benchmarks/...' in the test code resolve correctly.
        working-directory: ./pr_code
        run: pytest -n auto -v

      - name: Run and Compare Benchmarks
        if: github.event_name == 'pull_request'
        run: |
          # 1. Generate baseline benchmark from main_code.
          #    Run from within the main_code directory.
          echo "--- Running benchmark on main branch ---"
          (cd main_code && pytest --benchmark-only --benchmark-save=main --benchmark-storage=file://../benchmarks)
          
          # 2. Generate benchmark for this PR from pr_code.
          #    Run from within the pr_code directory.
          echo "--- Running benchmark on PR branch ---"
          (cd pr_code && pytest --benchmark-only --benchmark-save=pr --benchmark-storage=file://../benchmarks)

          # 3. Compare results. The storage path is relative to the root.
          echo "--- Comparing benchmarks ---"
          pytest --benchmark-compare --benchmark-storage=file://./benchmarks main:pr

      - name: Generate and Upload Report
        if: github.event_name == 'pull_request'
        run: python pr_code/generate_benchmark_report.py
      
      - name: Upload Report Artifacts
        if: github.event_name == 'pull_request'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: |
            benchmark_report.md
            benchmark_chart.png

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          path: benchmark_report.md
