name: Python CI & Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  # ====================================================================
  # JOB 1: Build and save a new performance baseline when code is merged into main.
  # This job's sole responsibility is to run on the main branch and store
  # its performance benchmark results as a workflow artifact.
  # ====================================================================
  build-and-save-baseline:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - name: Checkout main branch code
        uses: actions/checkout@v4
        with:
              submodules: true
              fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      - name: Install Hatch
        run: python -m pip install hatch

      - name: Run benchmark and save results
        run: hatch run bench main_baseline.json

      - name: Upload benchmark baseline as artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline-${{ matrix.python-version }}
          path: main_baseline.json


  # ====================================================================
  # JOB 2: For Pull Requests, using the official GitHub CLI for robust artifact handling.
  # This is the definitive version to solve the artifact download issues.
  # ====================================================================
  test-and-benchmark-pr:
    if: github.event_name == 'pull_request'
    permissions:
      # actions:read is crucial for gh run list to search workflow history.
      actions: read
      # pull-requests:write is for commenting.
      pull-requests: write
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      # --- Step 1: Prepare PR Environment ---
      - name: Checkout PR code
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libgmp-dev libmpfr-dev zlib1g-dev cmake
      - name: Install Hatch and run benchmark on PR
        run: |
          python -m pip install hatch
          hatch run bench pr_benchmark.json


      # --- Step 2: Get or Generate Baseline from Main using GitHub CLI ---
      - name: Get or Generate Baseline from Main
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          LATEST_RUN_ID=$(gh run list --workflow "${{ github.workflow }}" --branch main --status success --limit 1 --json databaseId -q '.[0].databaseId')

          if [[ -n "$LATEST_RUN_ID" && "$LATEST_RUN_ID" != "null" ]]; then
            echo "‚úÖ Found successful run: $LATEST_RUN_ID. Downloading artifact..."
            gh run download "$LATEST_RUN_ID" -n "benchmark-baseline-${{ matrix.python-version }}" --dir . || echo "Download failed, will proceed to fallback."
          else
            echo "INFO: No successful workflow run found."
          fi

          if [ ! -f "main_baseline.json" ]; then
            echo "‚ö†Ô∏è WARNING: Baseline not found. Generating from main branch on-the-fly."
            git clone "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}" main_code --branch main --single-branch
            cd main_code
            echo "INFO: Installing Hatch and running benchmark on main branch..."
            python -m pip install hatch
            hatch run bench ../main_baseline.json
            cd ..
          fi
          
          if [ ! -f "main_baseline.json" ]; then
             echo "‚ùå ERROR: Failed to generate a baseline. Cannot proceed."
             exit 1
          fi

      # --- Step 3: Compare (for CI failure check) and Report Generation ---
      - name: Compare for regression & determine status
        id: compare_benchmarks
        run: |
          echo "comparison_performed=true" >> $GITHUB_OUTPUT
          echo "INFO: Checking for performance regressions..."
          hatch run bench-compare main_baseline.json --fail=mean:10%

      - name: Generate benchmark markdown report
        run: |
          python scripts/generate_benchmark_report.py \
            --comparison-status ${{ steps.compare_benchmarks.outputs.comparison_performed }} \
            --main-file main_baseline.json \
            --pr-file pr_benchmark.json \
            --report-file benchmark_report.md
      
      - name: Upload benchmark report artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ matrix.python-version }}
          path: |
            benchmark_report.md
            pr_benchmark.json
            main_baseline.json

      - name: Comment benchmark results on PR
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          path: benchmark_report.md
          header: "üîç Benchmark Comparison Report (Python ${{ matrix.python-version }})"

